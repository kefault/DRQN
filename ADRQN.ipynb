{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghmiao/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample, randint, random\n",
    "import tensorflow.contrib.slim as slim\n",
    "import itertools as it\n",
    "from time import time, sleep\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import tensorflow as tf\n",
    "from tqdm import trange\n",
    "from vizdoom import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, memory_cap, batch_size, resolution, trace_length):\n",
    "\n",
    "        state_shape = (memory_cap, resolution[0], resolution[1], resolution[2])\n",
    "        self.s1 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.s2 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.a = np.zeros(memory_cap, dtype=np.int32)\n",
    "        self.pa = np.zeros(memory_cap, dtype=np.int32)\n",
    "        self.r = np.zeros(memory_cap, dtype=np.float32)\n",
    "        self.d = np.zeros(memory_cap, dtype=np.float32)\n",
    "\n",
    "        self.memory_cap = memory_cap\n",
    "        self.batch_size = batch_size\n",
    "        self.trace_length = trace_length\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def add_transition(self, s1, a, pa, r, s2, d):\n",
    "\n",
    "        self.s1[self.index, :, :, :] = s1\n",
    "        self.a[self.index] = a\n",
    "        self.pa[self.index] = pa\n",
    "        self.r[self.index] = r\n",
    "        self.s2[self.index, :, :, :] = s2\n",
    "        self.d[self.index] = d\n",
    "\n",
    "        self.index = (self.index+1) % self.memory_cap\n",
    "        self.size = min(self.size + 1, self.memory_cap)\n",
    "\n",
    "    def get_transition(self):\n",
    "        indexes = []\n",
    "        for _ in range(self.batch_size):\n",
    "            accepted = False\n",
    "            while not accepted:\n",
    "                point = np.random.randint(0, self.size - self.trace_length)\n",
    "                accepted = True\n",
    "                for i in range(self.trace_length-1):\n",
    "                    if self.d[point+i] > 0:\n",
    "                        accepted = False\n",
    "                        break\n",
    "                if accepted:\n",
    "                    for i in range(self.trace_length):\n",
    "                        indexes.append(point+i)\n",
    "\n",
    "        return self.s1[indexes], self.a[indexes], self.pa[indexes], self.r[indexes], self.s2[indexes], self.d[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, session, action_count, resolution, lr, batch_size, trace_length, hidden_size, scope):\n",
    "        self.session = session\n",
    "        self.resolution = resolution\n",
    "        self.train_batch_size = batch_size\n",
    "        self.trace_length_size = trace_length\n",
    "\n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, resolution[0], resolution[1], resolution[2]])\n",
    "\n",
    "        conv1 = slim.conv2d(inputs=self.state, num_outputs=32, kernel_size=[8, 8], stride=[4, 4],\n",
    "                            activation_fn=tf.nn.relu, padding='VALID', scope=scope+'_c1')\n",
    "\n",
    "        conv2 = slim.conv2d(inputs=conv1, num_outputs=64, kernel_size=[4, 4], stride=[2, 2],\n",
    "                            activation_fn=tf.nn.relu, padding='VALID', scope=scope+'_c2')\n",
    "\n",
    "        conv3 = slim.conv2d(inputs=conv2, num_outputs=64, kernel_size=[3, 3], stride=[1, 1],\n",
    "                            activation_fn=tf.nn.relu, padding='VALID', scope=scope+'_c3')\n",
    "\n",
    "        flat_obs = slim.flatten(conv3)\n",
    "        \n",
    "        #####################################\n",
    "        # 动作的embedding\n",
    "        self.prev_action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.prev_action_onehot = tf.one_hot(self.prev_action, action_count, dtype=tf.float32)\n",
    "        fc_action = slim.fully_connected(self.prev_action_onehot, 512, activation_fn=None, scope=scope)\n",
    "        flat_act = slim.flatten(fc_action)\n",
    "        #####################################\n",
    "        \n",
    "        self.cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "        self.train_length = tf.placeholder(dtype=tf.int32)\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "\n",
    "        self.fc_reshape_obs = tf.reshape(flat_obs, [self.batch_size, self.train_length, hidden_size])\n",
    "        #####################################\n",
    "        # reshpe动作，并把动作和观察链接\n",
    "        self.fc_reshape_act = tf.reshape(flat_act, [self.batch_size, self.train_length, 512])\n",
    "        self.fc_concat = tf.concat([self.fc_reshape_obs, self.fc_reshape_act], 2)\n",
    "        print('链接之后：', self.fc_concat)\n",
    "        #####################################\n",
    "        \n",
    "        self.state_in = self.cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn, self.rnn_state = tf.nn.dynamic_rnn(inputs=self.fc_concat, cell=self.cell, dtype=tf.float32,\n",
    "                                                     initial_state=self.state_in, scope=scope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn, shape=[-1, hidden_size])\n",
    "\n",
    "        self.q = slim.fully_connected(self.rnn, action_count, activation_fn=None)\n",
    "\n",
    "        self.best_a = tf.argmax(self.q, 1)\n",
    "\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, action_count, dtype=tf.float32)\n",
    "        self.q_chosen = tf.reduce_sum(tf.multiply(self.q, self.actions_onehot), axis=1)\n",
    "\n",
    "        self.loss = tf.losses.mean_squared_error(self.q_chosen, self.target_q)\n",
    "\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, momentum=0.95, epsilon=0.01)\n",
    "\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def learn(self, state, target_q, state_in, action, prev_action):\n",
    "        feed_dict = {self.state: state, self.target_q: target_q, self.train_length: self.trace_length_size, self.prev_action: prev_action,\n",
    "                     self.batch_size: self.train_batch_size, self.state_in: state_in, self.actions: action}\n",
    "        l, _ = self.session.run([self.loss, self.train_step], feed_dict=feed_dict)\n",
    "        return l\n",
    "\n",
    "    def get_q(self, state, state_in, prev_action):\n",
    "        return self.session.run(self.q, feed_dict={self.state: state, self.train_length: self.trace_length_size, self.prev_action: prev_action,\n",
    "                                                   self.batch_size: self.train_batch_size, self.state_in: state_in})\n",
    "\n",
    "    def get_best_action(self, state, state_in, prev_action):\n",
    "        return self.session.run([self.best_a, self.rnn_state], feed_dict={self.state: [state], self.train_length: 1, self.prev_action: [prev_action],\n",
    "                                                                          self.batch_size: 1, self.state_in: state_in})\n",
    "\n",
    "    def get_cell_state(self, state, state_in, prev_action):\n",
    "        return self.session.run(self.rnn_state, feed_dict={self.state: [state], self.train_length: 1, self.prev_action: prev_action,\n",
    "                                                           self.state_in: state_in, self.batch_size: 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, memory_cap, batch_size, resolution, action_count, session,\n",
    "                 lr, gamma, epsilon_min, epsilon_decay_steps, epsilon_max, trace_length, hidden_size):\n",
    "\n",
    "        self.model = Network(session=session, action_count=action_count,\n",
    "                             resolution=resolution, lr=lr, batch_size=batch_size,\n",
    "                             trace_length=trace_length, hidden_size=hidden_size, scope='main')\n",
    "        self.target_model = Network(session=session, action_count=action_count,\n",
    "                                    resolution=resolution, lr=lr, batch_size=batch_size,\n",
    "                                    trace_length=trace_length, hidden_size=hidden_size, scope='target')\n",
    "\n",
    "        self.memory = ReplayMemory(memory_cap=memory_cap, batch_size=batch_size,\n",
    "                                   resolution=resolution, trace_length=trace_length)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.action_count = action_count\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.hidden_size = hidden_size\n",
    "        self.trace_length = trace_length\n",
    "\n",
    "        self.epsilon = epsilon_max\n",
    "        self.training_steps = 0\n",
    "\n",
    "        self.epsilon_decrease = (epsilon_max-epsilon_min)/epsilon_decay_steps\n",
    "\n",
    "        self.min_buffer_size = batch_size*trace_length\n",
    "\n",
    "        self.state_in = (np.zeros([1, self.hidden_size]), np.zeros([1, self.hidden_size]))\n",
    "\n",
    "    def add_transition(self, s1, a, pa, r, s2, d):\n",
    "        self.memory.add_transition(s1, a, pa, r, s2, d)\n",
    "\n",
    "    def learn_from_memory(self):\n",
    "\n",
    "        if self.memory.size > self.min_buffer_size:\n",
    "            state_in = (np.zeros([self.batch_size, self.hidden_size]), np.zeros([self.batch_size, self.hidden_size]))\n",
    "            s1, a, pa, r, s2, d = self.memory.get_transition()\n",
    "            inputs = s1\n",
    "            \n",
    "            # 要修改这里，变成，上一步的action\n",
    "            q = np.max(self.target_model.get_q(s2, state_in, pa), axis=1)\n",
    "            targets = r + self.gamma * (1 - d) * q\n",
    "            \n",
    "            # 要修改这里，变成，上一步的action\n",
    "            self.model.learn(inputs, targets, state_in, a, pa)\n",
    "\n",
    "    def act(self, state, prev_action, train=True):\n",
    "        if train:\n",
    "            self.epsilon = self.explore(self.epsilon)\n",
    "            if random() < self.epsilon:\n",
    "                a = self.random_action()\n",
    "            else:\n",
    "                a, self.state_in = self.model.get_best_action(state, self.state_in, prev_action)\n",
    "                a = a[0]\n",
    "        else:\n",
    "            a, self.state_in = self.model.get_best_action(state, self.state_in, prev_action)\n",
    "            a = a[0]\n",
    "        return a\n",
    "\n",
    "    def explore(self, epsilon):\n",
    "        return max(self.epsilon_min, epsilon-self.epsilon_decrease)\n",
    "\n",
    "    def random_action(self):\n",
    "        return randint(0, self.action_count - 1)\n",
    "\n",
    "    def reset_cell_state(self):\n",
    "        self.state_in = (np.zeros([1, self.hidden_size]), np.zeros([1, self.hidden_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRAME_REPEAT = 4 # How many frames 1 action should be repeated\n",
    "UPDATE_FREQUENCY = 4 # How many actions should be taken between each network update\n",
    "\n",
    "RESOLUTION = (80, 45, 3) # Resolution\n",
    "BATCH_SIZE = 32 # Batch size for experience replay\n",
    "LEARNING_RATE = 0.00025 # Learning rate of model\n",
    "GAMMA = 0.99 # Discount factor\n",
    "\n",
    "MEMORY_CAP = 10000 # Amount of samples to store in memory\n",
    "\n",
    "EPSILON_MAX = 0.5 # Max exploration rate\n",
    "EPSILON_MIN = 0.1 # Min exploration rate\n",
    "EPSILON_DECAY_STEPS = 2e5 # How many steps to decay from max exploration to min exploration\n",
    "\n",
    "RANDOM_WANDER_STEPS = 500 # How many steps to be sampled randomly before training starts\n",
    "\n",
    "TRACE_LENGTH = 8 # How many traces are used for network updates\n",
    "HIDDEN_SIZE = 768 # Size of the third convolutional layer when flattened\n",
    "\n",
    "EPOCHS = 1#200 # Epochs for training (1 epoch = 10k training steps and 10 test episodes)\n",
    "STEPS_PER_EPOCH = 10000 # How actions to be taken per epoch\n",
    "EPISODES_TO_TEST = 10 # How many test episodes to be run per epoch for logging performance\n",
    "EPISODE_TO_WATCH = 10 # How many episodes to watch after training is complete\n",
    "\n",
    "TAU = 0.001 # How much the target network should be updated towards the online network at each update\n",
    "\n",
    "LOAD_MODEL = False # Load a saved model?\n",
    "SAVE_MODEL = False # Save a model while training?\n",
    "SKIP_LEARNING = False # Skip training completely and just watch?\n",
    "\n",
    "scenario_path = \"../../ViZDoom/scenarios/my_way_home.cfg\" # Name and path of scenario\n",
    "model_savefile = \"Models/MWH/model\" # Name and path of the model\n",
    "reward_savefile = \"Rewards_MWH.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "\n",
    "def initialize_vizdoom():\n",
    "    print(\"Initializing doom...\")\n",
    "    game = DoomGame()\n",
    "    game.load_config(scenario_path)\n",
    "    game.set_window_visible(False)\n",
    "    game.set_mode(Mode.PLAYER)\n",
    "    game.set_screen_format(ScreenFormat.RGB24)\n",
    "    game.set_screen_resolution(ScreenResolution.RES_400X225)\n",
    "    game.init()\n",
    "\n",
    "    print(\"Doom initialized.\")\n",
    "    return game\n",
    "\n",
    "def preprocess(img):\n",
    "    img = skimage.transform.resize(img, RESOLUTION, mode='constant')\n",
    "    img = img.astype(np.float32)\n",
    "    return img\n",
    "\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "\n",
    "def saveScore(score):\n",
    "    my_file = open(reward_savefile, 'a')  # Name and path of the reward text file\n",
    "    my_file.write(\"%s\\n\" % test_scores.mean())\n",
    "    my_file.close()\n",
    "\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n",
      "链接之后： Tensor(\"concat:0\", shape=(?, ?, 1280), dtype=float32)\n",
      "链接之后： Tensor(\"concat_2:0\", shape=(?, ?, 1280), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filling out replay memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 training episodes played.\n",
      "Results: mean: -0.1±0.3, min: -0.2, max: 1.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -0.2±0.0, min: -0.2 max: -0.2\n",
      "Total ellapsed time: 21.40 minutes\n",
      "TIME TO WATCH!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [09:11<01:01, 61.31s/it]"
     ]
    }
   ],
   "source": [
    "game = initialize_vizdoom()\n",
    "\n",
    "n = game.get_available_buttons_size()\n",
    "actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "ACTION_COUNT = len(actions)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.33)\n",
    "\n",
    "SESSION = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "agent = Agent(memory_cap = MEMORY_CAP, batch_size = BATCH_SIZE, resolution = RESOLUTION, action_count = ACTION_COUNT,\n",
    "            session = SESSION, lr = LEARNING_RATE, gamma = GAMMA, epsilon_min = EPSILON_MIN, trace_length=TRACE_LENGTH,\n",
    "            epsilon_decay_steps = EPSILON_DECAY_STEPS, epsilon_max=EPSILON_MAX, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables, TAU)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    print(\"Loading model from: \", model_savefile)\n",
    "    saver.restore(SESSION, model_savefile)\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "    SESSION.run(init)\n",
    "\n",
    "##########################################\n",
    "\n",
    "if not SKIP_LEARNING:\n",
    "    time_start = time()\n",
    "    print(\"\\nFilling out replay memory\")\n",
    "    updateTarget(targetOps, SESSION)\n",
    "\n",
    "    episode_buffer = []\n",
    "    agent.reset_cell_state()\n",
    "    state = preprocess(game.get_state().screen_buffer)\n",
    "    ######################################\n",
    "    # 添加了上一步动作，可能还要处理\n",
    "    prev_action = agent.random_action()\n",
    "    ######################################\n",
    "    for _ in trange(RANDOM_WANDER_STEPS, leave=False):\n",
    "        action = agent.random_action()\n",
    "        reward = game.make_action(actions[action], FRAME_REPEAT)\n",
    "        done = game.is_episode_finished()\n",
    "        if not done:\n",
    "            state_new = preprocess(game.get_state().screen_buffer)\n",
    "        else:\n",
    "            state_new = None\n",
    "\n",
    "        agent.add_transition(state, action, prev_action, reward, state_new, done)\n",
    "        state = state_new\n",
    "        ######################################\n",
    "        # 添加了上一步动作，可能还要处理\n",
    "        prev_action = action\n",
    "        ######################################\n",
    "        \n",
    "        if done:\n",
    "            game.new_episode()\n",
    "            agent.reset_cell_state()\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"\\n\\nEpoch %d\\n-------\" % (epoch + 1))\n",
    "\n",
    "        train_episodes_finished = 0\n",
    "        train_scores = []\n",
    "\n",
    "        print(\"Training...\")\n",
    "        game.new_episode()\n",
    "\n",
    "        episode_buffer = []\n",
    "        agent.reset_cell_state()\n",
    "        state = preprocess(game.get_state().screen_buffer)\n",
    "        for learning_step in trange(STEPS_PER_EPOCH, leave=False):\n",
    "            action = agent.act(state, prev_action)\n",
    "            reward = game.make_action(actions[action], FRAME_REPEAT)\n",
    "            done = game.is_episode_finished()\n",
    "            if not done:\n",
    "                state_new = preprocess(game.get_state().screen_buffer)\n",
    "            else:\n",
    "                state_new = None\n",
    "\n",
    "            agent.add_transition(state, action, prev_action, reward, state_new, done)\n",
    "            state = state_new\n",
    "            ######################################\n",
    "            # 添加了上一步动作，可能还要处理\n",
    "            prev_action = action\n",
    "            ######################################\n",
    "\n",
    "            if learning_step % UPDATE_FREQUENCY == 0:\n",
    "                agent.learn_from_memory()\n",
    "                updateTarget(targetOps, SESSION)\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(game.get_total_reward())\n",
    "                train_episodes_finished += 1\n",
    "                game.new_episode()\n",
    "                agent.reset_cell_state()\n",
    "                state = preprocess(game.get_state().screen_buffer)\n",
    "\n",
    "        print(\"%d training episodes played.\" % train_episodes_finished)\n",
    "        train_scores = np.array(train_scores)\n",
    "\n",
    "        print(\"Results: mean: %.1f±%.1f,\" % (train_scores.mean(), train_scores.std()),\n",
    "            \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())\n",
    "\n",
    "        print(\"\\nTesting...\")\n",
    "\n",
    "        test_scores = []\n",
    "        for test_step in trange(EPISODES_TO_TEST, leave=False):\n",
    "            game.new_episode()\n",
    "            prev_action = agent.random_action()\n",
    "            agent.reset_cell_state()\n",
    "            while not game.is_episode_finished():\n",
    "                state = preprocess(game.get_state().screen_buffer)\n",
    "                action = agent.act(state, prev_action, train=False)\n",
    "                prev_action = action\n",
    "                game.make_action(actions[action], FRAME_REPEAT)\n",
    "            test_scores.append(game.get_total_reward())\n",
    "\n",
    "        test_scores = np.array(test_scores)\n",
    "        print(\"Results: mean: %.1f±%.1f,\" % (test_scores.mean(), test_scores.std()),\n",
    "              \"min: %.1f\" % test_scores.min(), \"max: %.1f\" % test_scores.max())\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            saveScore(test_scores.mean())\n",
    "            saver.save(SESSION, model_savefile)\n",
    "            print(\"Saving the network weigths to:\", model_savefile)\n",
    "            if epoch % (EPOCHS/5) == 0 and epoch is not 0:\n",
    "                saver.save(SESSION, model_savefile, global_step=epoch)\n",
    "\n",
    "        print(\"Total ellapsed time: %.2f minutes\" % ((time() - time_start) / 60.0))\n",
    "\n",
    "print(\"TIME TO WATCH!!\")\n",
    "# Reinitialize the game with window visible\n",
    "game.close()\n",
    "game.set_window_visible(False)\n",
    "game.set_mode(Mode.ASYNC_PLAYER)\n",
    "game.init()\n",
    "score = []\n",
    "\n",
    "for _ in trange(EPISODE_TO_WATCH, leave=False):\n",
    "    game.new_episode()\n",
    "    prev_action = agent.random_action()\n",
    "    agent.reset_cell_state()\n",
    "    while not game.is_episode_finished():\n",
    "        state = preprocess(game.get_state().screen_buffer)\n",
    "        action = agent.act(state, prev_action, train=False)\n",
    "        prev_action = action\n",
    "        game.set_action(actions[action])\n",
    "        for i in range(FRAME_REPEAT):\n",
    "            game.advance_action()\n",
    "            done = game.is_episode_finished()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Sleep between episodes\n",
    "    sleep(1.0)\n",
    "    score.append(game.get_total_reward())\n",
    "score = np.array(score)\n",
    "game.close()\n",
    "print(\"Results: mean: %.1f±%.1f,\" % (score.mean(), score.std()),\n",
    "          \"min: %.1f\" % score.min(), \"max: %.1f\" % score.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
